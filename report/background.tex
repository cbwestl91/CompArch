During the last 30 years the CPU has evolved tremendously, increasing the instructions per second from 0.2 MIPS on the Intel 8088 in 1979\cite{reference:intel} to 177 730 MIPS on the Intel Core Extreme Edition i7-3960X in 2011 \cite{reference:inteli7}, while the performance of memory has barely increased in comparison. Because of this gap, the memory access speed is acting as a bottleneck in the pipeline, where the CPU has to wait for the fetch operation to finish. This wait is shortened a bit by launching new instructions that were determined to be uncorrelated to the queried memory fetch, but it doesn't make that much of a difference on overall performance.

The invention of virtual memory space allowed a memory hierarchy system to be implemented. This way large, inexpensive and slow memories like RAM can be combined with small, fast and expensive memory on chip memory. The desired memory is "cached" in the on chip memory in blocks, and the process can be accelerated significantly by only having to access on chip memory instead of RAM. If memory that is to be referenced together has addresses that are close in virtual memory, the probability of it already being cached increases. The probability of a cache hit depends heavily on cache size, and the size of the memory block that is cached each time the memory manager needs to access the RAM. In addition to the on chip and off chip memory split, it is common to have both two and three levels of data cache on chip, with L1 cache being the highest level, lying closest to the processing core, often local to the core it services. While this makes the performance a lot better than with off chip memory alone, it doesn't do much for the cases where the memory address isn't cached. The processor is still waiting for accesses, and the limited size of the cache is a concern (the aforementioned Intel i7-processor features a 15MB L3 cache, which puts it at the top of the desktop market). 

In an effort to reduce the wait associated with memory access on the off chip memory, the prefetcher attempts to determine the address of the memory that is to be accessed before it's actually needed. The prefetcher was first introduced on the Intel 8086, and monitors cache misses, attempting to detect access patterns in the form of independent, interwoven streams of addresses \cite{reference:8086}. A perfect prefetcher would assure that the processor always had the desired memory in cache at the right time, but as the memory address space granted to the prefetcher has to be minimal for it to have any benefit at all, the success is equally limited.\\
\begin{figure}[h!]
	\includegraphics[width=3.5in]{graphics/CPUmemoryGap.jpg}
	\caption{CPU/memory speed gap illustrated}
	\label{graph:cpugap}
\end{figure}
\\
